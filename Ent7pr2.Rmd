---
title: "Ent6Pr2"
author: "Javier Chiquín, Ricardo Morales"
date: "2025-04-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# Cargar librerías necesarias
library(caret)
library(e1071)
library(ggplot2)
library(dplyr)

# Cargar datos
data <- read.csv("C:\\Users\\javie\\Documents\\UVG\\Cuarto año\\Primer Semestre\\Mineria\\train.csv")

# Crear categorías de precio (consistentes con entregas anteriores)
set.seed(42) # Para reproducibilidad
percentiles <- quantile(data$SalePrice, probs = c(0.33, 0.66))
data$Price_Category <- cut(data$SalePrice,
                          breaks = c(-Inf, percentiles[1], percentiles[2], Inf),
                          labels = c('Economica', 'Media', 'Cara'))

# Selección de características (consistentes con entregas anteriores)
features <- c('GrLivArea', 'OverallQual', 'TotalBsmtSF', 'GarageCars', 'FullBath', 'YearBuilt')
X <- data[, features]
y <- data$Price_Category

# Crear particiones train-test (70-30) - MISMAS que en entregas anteriores
set.seed(42)
trainIndex <- createDataPartition(y, p = 0.7, list = FALSE)
X_train <- X[trainIndex, ]
X_test <- X[-trainIndex, ]
y_train <- y[trainIndex]
y_test <- y[-trainIndex]

```



```{r}
# Escalado de características (crucial para SVM)
preProc <- preProcess(X_train, method = c("center", "scale"))
X_train_scaled <- predict(preProc, X_train)
X_test_scaled <- predict(preProc, X_test)

# Verificar balance de clases
cat("\nDistribución de clases en entrenamiento:\n")
print(prop.table(table(y_train)))
```



```{r}
# Configuración común para todos los modelos
ctrl <- trainControl(method = "cv", number = 5, savePredictions = TRUE)

# a) SVM Lineal
set.seed(42)
svm_linear <- train(x = X_train_scaled, y = y_train,
                   method = "svmLinear",
                   trControl = ctrl,
                   tuneLength = 5)

# b) SVM Radial (RBF)
set.seed(42)
svm_radial <- train(x = X_train_scaled, y = y_train,
                   method = "svmRadial",
                   trControl = ctrl,
                   tuneLength = 5)

# c) SVM Polinomial
set.seed(42)
svm_poly <- train(x = X_train_scaled, y = y_train,
                 method = "svmPoly",
                 trControl = ctrl,
                 tuneLength = 3)

# d) SVM con búsqueda en grilla más exhaustiva (versión corregida para svmPoly)
grid <- expand.grid(degree = c(2, 3, 4),  # Grado del polinomio
                   scale = c(0.01, 0.1, 1),  # Parámetro de escala
                   C = c(0.1, 1, 10, 100))  # Parámetro de costo

set.seed(42)
svm_tuned <- train(x = X_train_scaled, y = y_train,
                  method = "svmPoly",
                  trControl = ctrl,
                  tuneGrid = grid)
```


```{r}
# Lista de modelos
models <- list(
  "SVM Lineal" = svm_linear,
  "SVM Radial" = svm_radial,
  "SVM Polinomial" = svm_poly,
  "SVM Tuneado" = svm_tuned
)

# Función para evaluar modelos
evaluate_model <- function(model, test_data, test_y) {
  start_time <- Sys.time()
  pred <- predict(model, test_data)
  time_taken <- Sys.time() - start_time
  
  cm <- confusionMatrix(pred, test_y)
  metrics <- data.frame(
    Accuracy = cm$overall["Accuracy"],
    Kappa = cm$overall["Kappa"],
    Time = as.numeric(time_taken)
  )
  
  return(list(metrics = metrics, cm = cm))
}

# Evaluar todos los modelos
results <- lapply(models, evaluate_model, X_test_scaled, y_test)

# Mostrar resultados
for (name in names(results)) {
  cat("\nModelo:", name, "\n")
  print(results[[name]]$metrics)
  cat("\nMatriz de confusión:\n")
  print(results[[name]]$cm$table)
}
```


```{r}
# Curvas de aprendizaje para el mejor modelo
best_model <- results[[which.max(sapply(results, function(x) x$metrics$Accuracy))]]

learning_curve <- function(model, data, y, train_sizes = seq(0.1, 1, 0.1), cv = 5) {
  ctrl <- trainControl(method = "cv", number = cv)
  lc <- learing_curve_dat(dat = cbind(data, Class = y),
                         outcome = "Class",
                         test_prop = 1 - min(train_sizes),
                         method = model$method,
                         metric = "Accuracy",
                         trControl = ctrl,
                         tuneGrid = model$bestTune)
  return(lc)
}

lc_data <- learning_curve(best_model, X_train_scaled, y_train)

ggplot(lc_data, aes(x = Training_Size, y = Accuracy, color = Data)) +
  geom_smooth(method = "loess") +
  labs(title = "Curva de aprendizaje del mejor modelo SVM",
       x = "Tamaño del conjunto de entrenamiento",
       y = "Precisión") +
  theme_minimal()
```


```{r}
# Cargar resultados de entregas anteriores (simulados)
previous_results <- list(
  "Árbol de Decisión" = list(Accuracy = 0.82, Kappa = 0.73, Time = 0.5),
  "Random Forest" = list(Accuracy = 0.87, Kappa = 0.80, Time = 5.2),
  "Naive Bayes" = list(Accuracy = 0.75, Kappa = 0.62, Time = 0.1),
  "KNN" = list(Accuracy = 0.80, Kappa = 0.70, Time = 0.3),
  "Regresión Logística" = list(Accuracy = 0.84, Kappa = 0.76, Time = 0.8)
)

# Combinar resultados
all_results <- c(lapply(results, function(x) x$metrics), previous_results)

# Crear dataframe comparativo
comparison_df <- do.call(rbind, all_results)
comparison_df$Model <- rownames(comparison_df)
rownames(comparison_df) <- NULL

# Visualización comparativa
ggplot(comparison_df, aes(x = reorder(Model, Accuracy), y = Accuracy, fill = Model)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Comparación de precisión entre modelos",
       x = "Modelo",
       y = "Precisión") +
  theme_minimal() +
  theme(legend.position = "none")

ggplot(comparison_df, aes(x = reorder(Model, Time), y = Time, fill = Model)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Comparación de tiempo de ejecución",
       x = "Modelo",
       y = "Tiempo (segundos)") +
  theme_minimal() +
  theme(legend.position = "none")
```
